High-level Design
=================

API Monitoring project aims to enable permanent monitoring of the public APIs
of an OpenStack-base platform, but due to using Ansible to execute individual
test it also may be used for any other activities. In the core of the system an
`executor` component is located, which is triggering pemanent execution of
ansible playbook test `scenarios`, which are located in the given git
repository. Data generated by the playbooks is then being sent as metrics to
the `telegraf` instance in any supported format. `telegraf` is then responsible
for sending data to a `influxDB` instance. This data is unique for each
enviroment where it is working. It is also being consumed by a clustered
`grafana` to present the test results.

::

    +----------------------------------------------------------------------------------------+
    | Environment 1                                                                          |
    |  +---------+     +---------+     +---------+               +--------+     +----------+ |
    |  |Executor |---->|Telegraf |---->|InfluxDB | ------------->|Grafana |---->|Clustered | |
    |  +---------+     +---------+     +---------+ \            >+--------+     |Grafana   | |
    |                                               -\        -/                |Database  | |
    +----------------------------------------------------------------------------------------+
                                                        -\-/                    |          |
                                                        -/-\                    |          |
    +----------------------------------------------------------------------------------------+
    |                                               -/        -\                |          | |
    |  +---------+     +---------+     +---------+-/            >+--------+     |          | |
    |  |Executor |---->|Telegraf |---->|InfluxDB | ------------->|Grafana |---->|          | |
    |  +---------+     +---------+     +---------+               +--------+     |          | |
    | Environment 2                                                             +----------+ |
    +----------------------------------------------------------------------------------------+


Ensuring, that the API works from inside of the platform make not that much
sense without ensuring that it is also working from outside the platform. To
ensure this a similar setup is being deployed as other environment outside of
the platform, but still uses the API of this platform to provision resourses.


Executor
--------

Executor is a component of the APImon system, which is responsible for
scheduling and executing individual jobs defined as ansible playbooks in the
configured repository. It is implemented as a process, which periodically scans
the repository and for each found "scenario" playbook it forks a process, which
will endlessly repeat it (probably with some delay, if required). Those
processes might be generating metrics in two different ways:
- undelaying playbook exposes metrics from the used libraries
- ansible plugins exposes additional metrics (i.e. whether the overall
  scenario succedded or not)


In the case of monitoring OpenStack APIs a functionality of OpenStackSDK
library used by ansible modules to export metrics on each individual executed
API call is exposed. This requires some special configuration in the
clouds.yaml file (currently exposing metrics into statsd and influxdb is
supported). For details please refer to the documentation of the OpenStackSDK.

Since in complex cases it might not be sufficient only to know the timings of
each individual made call ansible callback can be implemented to report overall
execution time and result (whether the overall scenario succeded and how long
did it took).


Telegraf
--------

The executor is exposing metrics, but where do the go? One possibility is to
place instance of `telegraf` to accept metrics from the `executor` and serve as
a proxy to place data (with potentially format conversion) into a required
destination. In out case it is proxying influxdb-format inserts into the real
database, which might require special access. In addition it immediately gives
possibility to expose data to `prometheus` instance (what is not currently
used).


InfluxDB
--------

InfluxDB OSS is used to store data of each individual APImon environment. It
receives data from `telegraf` and expose it to `grafana`.


Grafana
-------

Having clustered `grafana` allows pemanently monitoring the platform from
differen places. Performance of the server provisioning from inside of the
platform can be compared from inside of an instance already running in the
cloud (you have an instance in cloud and would like to create another one from
it) with doing that through a real internet connection. This helps to detect
potential problems with API-gateways, internet channels (an issue we have seen
ourselves). In `grafana` we can then implement dashboard with panels showing
the same measures from different datasources and immediately see a problem.

Grafana is component of the monitoring system, which requires a proper
failover. It can be implemented in different ways with a real load-balancer
instance, DNS with load-balancer, DNS round-robbin, etc. We currently do this
as a DNS with round-robin switching between different environments.


InfluxDB vs Prometheus
----------------------

Prometheus is a nice tools, but the nature of the API monitoring from the
customer point of view is to periodically try to invoke API. Those calls by nature
might have different duration and trying to estimate some average value for the
last 5 minutes is a wrong approach. Instead what we do is we generate events.
Those events should be saved in any kind of database (preferably time-series
DB).



